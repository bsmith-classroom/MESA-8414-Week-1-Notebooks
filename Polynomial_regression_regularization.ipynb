{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning, Week 1 Exercises.\n",
    "## Regularization examples with varying lambda values\n",
    "This notebook creates a small model and shows how it varies when using L1 and L2 regularization. It also shows how changing the value of the `lambda` (λ) parameter in L1 and L2 regularization shrink the model coefficients and reduce the changes of overfitting the model.\n",
    "\n",
    "This code is modified from the excellent [example](https://github.com/luisguiserrano/manning/tree/master/Chapter_04_Testing_Overfitting_Underfitting) in Chapter 4 of Luis Serrano's [_Grokking Machine Learning_](https://www.manning.com/books/grokking-machine-learning). The major difference is some additional examples at the end that show how lasso and ridge regression change with different lambda values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import utils\n",
    "\n",
    "# set the random seed to 0 to make the result reproducible\n",
    "# uncomment this or change the seed to see different results\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the dataset\n",
    "We'll create a set of random data points close to an inverted parabola ($y = -x^2+2$). First, generate the parabola and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our original polynomial is -x^2+2, so the coefficient list = [2, 0, -1]\n",
    "# (the 0th coefficient is the 0th in the array, the 1st is the 1st...)\n",
    "coefs = [2, 0,  -1]\n",
    "\n",
    "def polynomial(coefs, x):\n",
    "    \"\"\"\n",
    "    Given an x value and a set of coefficients, return the y value for the polynomial.\n",
    "\n",
    "    Args:\n",
    "        coefs: a list of coefficients\n",
    "        x: the x value in the polynomial\n",
    "\n",
    "    Returns:\n",
    "        the sum (y) of multiplying each coefficient by x\n",
    "    \"\"\"\n",
    "    n = len(coefs)\n",
    "    return sum([coefs[i]*x**i for i in range(n)])\n",
    "\n",
    "def draw_polynomial(coefs, linestyle=\"-\", label=\"\"):\n",
    "    \"\"\"\n",
    "        Draw a polynomial with the set of coefficients\n",
    "\n",
    "        Args:\n",
    "            a set of polynomial coefficients\n",
    "    \"\"\"\n",
    "    n = len(coefs)\n",
    "    x = np.linspace(-1, 1, 1000)\n",
    "    #plt.ylim(0,3)\n",
    "    plt.title(r\"Parabola with equation $y = -x^2+2$ \")\n",
    "    plt.plot(x, sum([coefs[i]*x**i for i in range(n)]), linestyle=linestyle, color='black', label=label)\n",
    "\n",
    "draw_polynomial(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little syntactic sugar. Make a function called inverted_parabola that \n",
    "# calls polynomial with our coefficients and an argument of x\n",
    "inverted_parabola = lambda x: polynomial(coefs,x)\n",
    "\n",
    "# calling inverted parabola with x=3 should give the y value of 7 (-3^2 + 2)\n",
    "assert inverted_parabola(3) == -7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a dataset of 40 points that lie close to this polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and Y will contain the (x,y) points for the data\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# generate 40 data points\n",
    "for i in range(40):\n",
    "\n",
    "    # pick a random x point\n",
    "    x = random.uniform(-1,1)\n",
    "\n",
    "    # use the x point to generate the y point and throw in randomness in to vary its value\n",
    "    y = inverted_parabola(x) + random.gauss(0,0.1)\n",
    "    X.append(x)\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the generated data against the \"ideal\" inverted parabola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, label=\"data\")\n",
    "draw_polynomial(coefs, label=r'$y = -x^2+2$', linestyle=\"dotted\" )\n",
    "plt.title(r\"40 data points generated around $y = -x^2+2$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a polynomial regression model\n",
    "\n",
    "First, define a function to train a model with 1) linear regression, 2) lasso regression, or 3) ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def train_polynomial_regression(X, Y, degree, regularization=None, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Trains a polynomial regression model with optional L1 or L2 regularization.\n",
    "\n",
    "    Args:\n",
    "      X: Input features (list or numpy array).\n",
    "      Y: Input labels (list or numpy array).\n",
    "      degree: The degree of the polynomial.\n",
    "      regularization: Type of regularization ('l1', 'l2', or None). Defaults to None.\n",
    "      alpha: Regularization strength (for L1 and L2). Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "      A trained scikit-learn model object (LinearRegression, Ridge, or Lasso).\n",
    "    \"\"\"\n",
    "    X = np.array(X).reshape(-1, 1)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "\n",
    "    if regularization == 'L1':\n",
    "        model = Lasso(alpha=alpha)\n",
    "    elif regularization == 'L2':\n",
    "        model = Ridge(alpha=alpha)\n",
    "    else:\n",
    "        model = LinearRegression()\n",
    "\n",
    "    model.fit(X_poly, Y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the function with the parameter `degrees_used`, a variable that contains the number of coefficents for the model. The example below sets `degrees_used = 10`, making it a polynomial with degree 10. This leads to overfitting (see the resulting plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_used = 10\n",
    "model = train_polynomial_regression(X, Y, degree_used)\n",
    "\n",
    "# plot the polynomial \n",
    "utils.plot_polynomial_regression(model, X, Y, degree_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into training and testing\n",
    "Split the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Shape of X_train:\", np.shape(X_train))\n",
    "print(\"Shape of X_test:\", np.shape(X_test))\n",
    "print(\"Shape of Y_train:\", np.shape(Y_train))\n",
    "print(\"Shape of Y_test:\", np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a polynomial regression model with no regularization\n",
    "This is standard linear regression. Train the model with `degree_used` coefficients using the training sets, then look at what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_reg = train_polynomial_regression(X_train, Y_train, degree_used)\n",
    "utils.plot_polynomial_regression(model_no_reg, X, Y, degree_used, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the root mean squared error (RMSE) using the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, degree):\n",
    "    \"\"\"\n",
    "    Evaluates a trained polynomial regression model on test data and returns the RMSE.\n",
    "\n",
    "    Args:\n",
    "      model: The trained LinearRegression model object.\n",
    "      X_test: Test set features (list or numpy array).\n",
    "      Y_test: Test set labels (list or numpy array).\n",
    "      degree: The degree of the polynomial used for training.\n",
    "\n",
    "    Returns:\n",
    "      The Root Mean Squared Error (RMSE) on the test set.\n",
    "    \"\"\"\n",
    "    X_test = np.array(X_test).reshape(-1, 1)\n",
    "    Y_test = np.array(Y_test)\n",
    "\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_test_poly = poly.fit_transform(X_test)\n",
    "\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test, y_pred))\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_loss_no_reg = evaluate_model(model_no_reg, X_test, Y_test, degree_used)\n",
    "print(f\"Square loss on the test set (degree {degree_used}): {square_loss_no_reg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a polynomial regression model with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Lambda = 0.01\n",
    "L1_lambda = 0.01\n",
    "\n",
    "# Train with L1 (Lasso) regularization\n",
    "model_L1_reg = train_polynomial_regression(X_train, Y_train, degree_used, 'L1', L1_lambda)\n",
    "utils.plot_polynomial_regression(model_L1_reg, X_train, Y_train, degree_used, X_test, Y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_loss_L1_reg = evaluate_model(model_L1_reg, X_test, Y_test, degree_used)\n",
    "print(f\"Square loss on the test set (degree {degree_used}): {square_loss_L1_reg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a polynomial regression model with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Lambda = 0.01\n",
    "L2_lambda = 0.01\n",
    "\n",
    "# Train with L2 (Ridge) regularization\n",
    "model_L2_reg = train_polynomial_regression(X_train, Y_train, degree_used, 'L2', L2_lambda)\n",
    "utils.plot_polynomial_regression(model_L2_reg, X_train, Y_train, degree_used, X_test, Y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_loss_L2_reg = evaluate_model(model_L2_reg, X_test, Y_test, degree_used)\n",
    "print(f\"Square loss on the test set (degree {degree_used}): {square_loss_L2_reg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the model coefficents\n",
    "\n",
    "Let's take a look at the coeeficients for each of the models: Linear, Lasso, and Ridge regression.   \n",
    "\n",
    "We should see three things:\n",
    "1. In the linear regression, all 10 of the model coeeficients have a non-zero value &mdash; all coefficients are included in the model.\n",
    "2. In the lasso regression, we end up with fewer coefficients than the original 10. This is because lasso, L1, regularization transforms some of the coefficients to zero, making for a simpler model.\n",
    "3. In the ridge regression, the 10 coefficients are likely still present but their values have been reduced. This tends to reduce some of the oscillations that can lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients of the model with no regularization\")\n",
    "print(model_no_reg.coef_)\n",
    "print()\n",
    "print(\"Coefficients of the model with L1 regularization\")\n",
    "print(model_L1_reg.coef_)\n",
    "print()\n",
    "print(\"Coefficients of the model with L2 regularization\")\n",
    "print(model_L2_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when lambda varies for L1 regularization?\n",
    "The code below generates four plots using L1 regularization where lambda = 0.0, 0.01, 0.05, and 0.1. When lambda = 0, the model is a standard linear regression, so you'll see the overfitting. As lambda increases, you start to see the model behavior changing. The fit looks quite good when lambda = 0.01. The parabola gets flatter when lambda = 0.05, and it's essentially linear at lambda = 0.1. This shows the importance of testing various values for lambda.\n",
    "\n",
    "If you look at the code, you'll see it doesn't use the `L1` parameter in `train_polynomial_regression` when `lambda = 0`. This is because scikit-learn's Lasso object doesn't like lambda = 0, and the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) tells us to use the LinearRegression object instead. That is because lambda=0 cancels out the regularization term, effectively making it a standard linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_values = [0.00, 0.01, 0.05, 0.1]\n",
    "\n",
    "# collect the models generated with different lambdas\n",
    "L1_models = []\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,8))\n",
    "\n",
    "# flatten the axs array to simplify the iteration in the for loop \n",
    "axs = axs.flatten()\n",
    "\n",
    "# plot each model with a different lambda value\n",
    "for i, lam in enumerate(lambda_values):\n",
    "    # Train with L1 (Lasso) regularization.\n",
    "    # Setting lambda = 0 with scikit-learn Lasso object isn't advised. Instead, we call the LinearRegression model\n",
    "    # since Lasso with λ = 0 is equivalent.\n",
    "    if lam == 0:\n",
    "        model_L1_reg = train_polynomial_regression(X_train, Y_train, degree_used, 'None') \n",
    "    else:\n",
    "        model_L1_reg = train_polynomial_regression(X_train, Y_train, degree_used, 'L1', lam) \n",
    "\n",
    "    L1_models.append(model_L1_reg)\n",
    "\n",
    "    # set the current axis for the plot    \n",
    "    plt.sca(axs[i])\n",
    "    utils.plot_polynomial_regression(model_L1_reg, X_train, Y_train, degree_used, X_test, Y_test)\n",
    "    axs[i].set_title(f\"Model with λ = {lam}\")\n",
    "\n",
    "\n",
    "fig.suptitle(\"L1 Regression Models with Varying λ\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the coefficients as λ increases during L1 regularization\n",
    "In L1 regularization, we should see the model coefficients getting smaller as lambda gets bigger. Even more characteristic of L1, some of the coefficients may disappear entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lambda_value, model in zip(lambda_values, L1_models):\n",
    "    print(f\"Coefficients of the model with L1 regularization and λ = {lambda_value}:\")\n",
    "    print(model.coef_, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when lambda varies for L2 regularization?\n",
    "The code below generates four plots where lambda = 0.0, 0.01, 0.1, and 1.0. When lambda = 0, the model is a standard linear regression, so you'll see the overfitting. As lambda increases, you start to see the model results changing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_values = [0.0, 0.01, 0.1, 1]\n",
    "\n",
    "# collect the models generated with different lambdas\n",
    "L2_models = []\n",
    "\n",
    "# set up the visualization for 4 subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,8))\n",
    "\n",
    "# flatten the axs array to simplify the iteration in the for loop \n",
    "axs = axs.flatten()\n",
    "\n",
    "# plot each model with a different lambda value\n",
    "for i, lam in enumerate(lambda_values):\n",
    "    # Train with L2 (Ridge) regularization\n",
    "    model_L2_reg = train_polynomial_regression(X_train, Y_train, degree_used, 'L2', lam)\n",
    "\n",
    "    L2_models.append(model_L2_reg)\n",
    "\n",
    "    # set the current axis for the plot    \n",
    "    plt.sca(axs[i])\n",
    "    \n",
    "    # plot the model, training daya, and test data\n",
    "    utils.plot_polynomial_regression(model_L2_reg, X_train, Y_train, degree_used, X_test, Y_test)\n",
    "\n",
    "    # set the subplot title\n",
    "    axs[i].set_title(f\"Model with λ = {lam}\")\n",
    "\n",
    "\n",
    "fig.suptitle(\"L2 Regression Models with Varying λ\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the coefficients as λ increases during L2 regularization\n",
    "In L2 regularization, we should also see the model coefficients getting smaller as lambda gets bigger. Unlike L1 regularization, i'ts unlikely that we'll see coefficients reduced completely to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lambda_value, model in zip(lambda_values, L2_models):\n",
    "    print(f\"Coefficients of the model with L2 regularization and λ = {lambda_value}:\")\n",
    "    print(model.coef_, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
